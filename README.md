Nuga Bank - Data Cleaning & Exploration with PySpark
Hey there!

This repository showcases a PySpark project I built to streamline Nuga Bank's data exploration and cleaning processes. Here's a quick rundown of what you'll find:

The Challenge:

Nuga Bank was struggling with:

Manual data exploration and cleaning :)
Ever-growing data volumes causing scalability issues
Inconsistent data leading to inaccurate reports and analysis
Transforming messy raw data into a structured format
The Solution:

I leveraged PySpark's distributed computing power to:

Automate data exploration and cleaning (bye-bye tedious tasks!)
Normalize the data for better integrity (think 2NF or 3NF)
Load the cleaned and normalized data into PostgreSQL for further analysis
Tech Stack:

Programming Languages: Python & SQL
Data Processing Framework: PySpark
Database: PostgreSQL Server
Project Highlights:


view the spark ui when i was creatig the code screenshot
<img width="1680" alt="spark ui engine view" src="https://github.com/user-attachments/assets/afb88736-4d1b-4ccc-a733-5e5767150c3b">

Efficiency Boost: Automation saved us tons of time and effort.
Scalability Power: PySpark handled massive datasets with ease.
Data Quality Champion: Standardized cleaning ensured consistent data.
Structured Bliss: Normalized data made querying and management a breeze.
Collaboration FTW: Streamlined workflows fostered better teamwork.
What's Included:

This repository delves into the project details, including:

Data Extraction: Setting up Spark and loading CSV data.
Data Transformation: Cleaning, handling missing values, duplicates, inconsistencies, and normalization.
Data Loading: Sending the cleaned data to PostgreSQL.
Feel free to:

Explore the code and adapt it to your specific needs.
Reach out if you have any questions!
Let's clean and explore data efficiently!
